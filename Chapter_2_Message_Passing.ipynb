{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLw9SYwU0Ls0r5onBEaQCO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zmgy107/DGL-Learning-Notes/blob/main/Chapter_2_Message_Passing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install  dgl -f https://data.dgl.ai/wheels/cu117/repo.html\n",
        "%pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html\n",
        "\n",
        "import dgl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zM7IU1a-qCE",
        "outputId": "ca6b424b-6a0d-47ac-98bc-786aa5e1ad89"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.dgl.ai/wheels/cu117/repo.html\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.9/dist-packages (1.0.1+cu117)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.9/dist-packages (from dgl) (3.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (5.9.4)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.dgl.ai/wheels-test/repo.html\n",
            "Collecting dglgo\n",
            "  Using cached dglgo-0.0.2-py3-none-any.whl (63 kB)\n",
            "Collecting numpydoc>=1.1.0\n",
            "  Downloading numpydoc-1.5.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from dglgo) (1.10.6)\n",
            "Requirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dglgo) (0.7.0)\n",
            "Collecting ogb>=1.3.3\n",
            "  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruamel.yaml>=0.17.20\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isort>=5.10.1\n",
            "  Downloading isort-5.12.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autopep8>=1.6.0\n",
            "  Downloading autopep8-2.0.2-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from dglgo) (1.2.2)\n",
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.9/dist-packages (from dglgo) (6.0)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.9/dist-packages (from autopep8>=1.6.0->dglgo) (2.0.1)\n",
            "Collecting pycodestyle>=2.10.0\n",
            "  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx>=4.2\n",
            "  Downloading sphinx-6.1.3-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.9/dist-packages (from numpydoc>=1.1.0->dglgo) (3.1.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from ogb>=1.3.3->dglgo) (1.13.1+cu116)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb>=1.3.3->dglgo) (1.26.15)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb>=1.3.3->dglgo) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from ogb>=1.3.3->dglgo) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.9/dist-packages (from ogb>=1.3.3->dglgo) (4.65.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from ogb>=1.3.3->dglgo) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic>=1.9.0->dglgo) (4.5.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.4/519.4 KB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->dglgo) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.10.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer>=0.4.0->dglgo) (8.1.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from rdkit-pypi->dglgo) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.2)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (63.4.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.1.5)\n",
            "Collecting docutils<0.20,>=0.18\n",
            "  Downloading docutils-0.19-py3-none-any.whl (570 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.5/570.5 KB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.4.1)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (23.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (6.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: snowballstemmer>=2.0 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.9 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.12.1)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.3)\n",
            "Collecting Pygments>=2.13\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.9/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (0.7.13)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8->sphinx>=4.2->numpydoc>=1.1.0->dglgo) (3.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.4)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=ea310006bc9908a8ed0d6c0f9d5818e8630240eca4735ebabfd5757125a5b988\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/bb/0d/2d02ec45f29c48d6192476bfb59c5a0e64b605e7212374dd15\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, ruamel.yaml.clib, rdkit-pypi, Pygments, pycodestyle, isort, docutils, sphinx, ruamel.yaml, outdated, autopep8, ogb, numpydoc, dglgo\n",
            "  Attempting uninstall: Pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.16\n",
            "    Uninstalling docutils-0.16:\n",
            "      Successfully uninstalled docutils-0.16\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 3.5.4\n",
            "    Uninstalling Sphinx-3.5.4:\n",
            "      Successfully uninstalled Sphinx-3.5.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pygments-2.14.0 autopep8-2.0.2 dglgo-0.0.2 docutils-0.19 isort-5.12.0 littleutils-0.2.2 numpydoc-1.5.0 ogb-1.3.5 outdated-0.2.2 pycodestyle-2.10.0 rdkit-pypi-2022.9.5 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7 sphinx-6.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Message Passing Paradigm\n",
        "\n",
        "Let $x_v\\in R^{d_1}$ be the function for node v ,and $w_e \\in R^{d_2}$ be the feature for edge (u,v). The __massage passing paradigm__ defines the following node-wise and edge-wise computation at step t+1:\n",
        "\n",
        "$ Edge-wise:m_e^{(t+1)}=\\phi(x_v^{(t)},x_u^{(t)},w_e^{(t)}),(u,v,e)\\inϵ.$\n",
        "\n",
        "$ Node-wise:x_v^{(t+1)}=\\psi(x_v^{(t)},\\rho(\\{m_e^{(t+1)}:(u,v,e)\\inϵ\\}))$.\n",
        "\n",
        "In the above equations,$\\phi$ is a __message function__ defined on each edge to generate a message by combining the edge feature with the features of its incident nodes;$\\varphi$ is an __update function__ defined on each node to update the node feature by aggregating its incoming messages using the __reduce function $\\rho$__"
      ],
      "metadata": {
        "id": "arp0ow2G1Db5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Built-in Functions and Message Passing APIs"
      ],
      "metadata": {
        "id": "NH8AzcqQL8YD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In DGL,__message function__ takes a single argument edges,which is an EdgeBatch instance.During message passing,DGL generates it internally to rpresent a batch of edge.It has three members __src,dst and data__ to access features of source nodes,destination nodes,and edges,repectively.\n",
        "\n",
        "__reduce function__ takes a single argument nodes,which is a NodeBatch instance.During message passing,DGL generates it internally to represent a batch of nodes.It has member mailbox to access the message received for the nodes in the batch.Some of the most common reduce operations include sum,max,min,etc.\n",
        "\n",
        "__update function__ takes a single argument nodes as described above.This function __operates on the aggregation result from reduce function__,typically combining it with a node's original feature at the last step and saving the result as a node feature.\n",
        "\n",
        "DGL has implemented commomly used message functions and reduce functions as __built-in__ in the namespace dgl.function. In general,DGL suggests using built-in functions __whenever possible__ since they are heavily optimized and automatically handle dimension broadcasting.\n",
        "\n",
        "Also can implement user-defined message/reduce function(UDF) when your message passing functions cannot be implemented with built-ins.\n",
        "\n",
        "Built-in message functions can be unary or binary. DGL supports copy for unary. For binary funcs, DGL supports __add, sub, mul, div, dot_. The naming convention for message built-in funcs is that u represents src nodes, v represents dst nodes, and e represents edges. The parameters for those functions are __strings__ indicating the input and output field names for the corresponding nodes and edges. The list of supported built-in functions can be found in DGL Built-in Function."
      ],
      "metadata": {
        "id": "miicakwT3l67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, to add the hu feature from src nodes and hv feature from dst nodes then save the result on the edge at he field, one can use built-in function dgl.function.u_add_v('hu', 'hv', 'he'). This is equivalent to the Message UDF:"
      ],
      "metadata": {
        "id": "i2-IiRxw8gvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def message_func(edges):\n",
        "  return {'he':edges.scr['hu']+edges.dst['hv']}"
      ],
      "metadata": {
        "id": "8eNOnGrQ8isM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Built-in reduce functions support operations sum, max, min, and mean. Reduce functions usually have two parameters, one for field name in mailbox, one for field name in node features, both are strings. For example, dgl.function.sum('m', 'h') is equivalent to the Reduce UDF that sums up the message m:"
      ],
      "metadata": {
        "id": "UA9_gvh29cbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def reduce_func(nodes):\n",
        "  return {'h':torch.sum(nodes.mailbox['m'],dim=1)}"
      ],
      "metadata": {
        "id": "Q-X-g8E49qlc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also possible to invoke only edge-wise computation by apply_edges() without invoking message passing. apply_edges() takes a message function for parameter and by default updates the features of all edges. For example:"
      ],
      "metadata": {
        "id": "pE87Uk15-XYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl.function as fn\n",
        "#graph.apply_edges(fn.u_add_v('el','er','e'))"
      ],
      "metadata": {
        "id": "U2a4xciq-aY8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For message passing, update_all() is a high-level API that __merges message generation, message aggregation and node update__ in a single call, which leaves room for optimization as a whole.\n",
        "\n",
        "The parameters for update_all() are a message function, a reduce function and an update function. One can call update function outside of update_all and not specify it in invoking update_all(). DGL recommends this approach since the update function can usually be written as pure tensor operations to make the code concise. For example："
      ],
      "metadata": {
        "id": "Fa5DpQO3-aEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_all_example(graph):\n",
        "  # store the result in graph.ndata['ft']\n",
        "  graph.update_all(fn.u_mul_v('ft','a','m'),fn.sum('m','ft'))\n",
        "  #Call update function outside of update_all\n",
        "  final_ft=graph.ndata['ft']*2\n",
        "  return final_ft"
      ],
      "metadata": {
        "id": "peX85GHkUoC8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This call will generate the messages m by multiply src node features ft and edge features a, sum up the messages m to update node features ft, and finally multiply ft by 2 to get the result final_ft. After the call, DGL will clean the intermediate messages m. The math formula for the above function is:\n",
        "\n",
        "$final\\_ft_i=2*\\sum_{j\\in N(i)}(fy_j*a_{ij})$\n",
        "\n",
        "DGL’s built-in functions support floating point data types, i.e. the feature must be half (float16) /float/double tensors. float16 data type support is disabled by default as it has a minimum GPU compute capacity requirement of sm_53 (Pascal, Volta, Turing and Ampere architectures)."
      ],
      "metadata": {
        "id": "VvrVQ9drWl8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 Writing Efficient Message Passing Code\n"
      ],
      "metadata": {
        "id": "d9bnLUcOXJp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DGL optimizes memory consumption and computing speed for message passing. A common practise to leverage those optimizations is to construct one’s own message passing functionality as a combination of update_all() calls with built-in functions as parameters.\n",
        "\n",
        "Besides that, considering that the number of edges is much larger than the number of nodes for some graphs, avoiding unnecessary memory copy from nodes to edges is beneficial. For some cases like GATConv, where it is necessary to save message on the edges, one needs to call apply_edges() with built-in functions. Sometimes the messages on the edges can be high dimensional, which is memory consuming. DGL recommends keeping the dimension of edge features as low as possible.\n",
        "\n",
        "Here’s an example on how to achieve this by splitting operations on the edges to nodes. The approach does the following: concatenate the src feature and dst feature, then apply a linear layer, i.e. W×(u||v)\n",
        ". The src and dst feature dimension is high, while the linear layer output dimension is low. A straight forward implementation would be like:"
      ],
      "metadata": {
        "id": "OwuKOAuTYEgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# linear = nn.Parameter(torch.FloatTensor(size=(node_feat_dim * 2, out_dim)))\n",
        "# def concat_message_function(edges):\n",
        "#     return {'cat_feat': torch.cat([edges.src['feat'], edges.dst['feat']], dim=1)}\n",
        "# g.apply_edges(concat_message_function)\n",
        "# g.edata['out'] = g.edata['cat_feat'] @ linear"
      ],
      "metadata": {
        "id": "4vDzlgEyBNxg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# realization\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "u,v=torch.tensor([0,1,2,3]),torch.tensor([2,0,0,1])\n",
        "weights=torch.tensor([0.1,0.6,0.9,0.7]) #weight of each edge\n",
        "g=dgl.graph((u,v))\n",
        "g.edata['w']=weights\n",
        "g.ndata['x']=torch.ones(g.num_nodes(),3)\n",
        "\n",
        "# linear=nn.Parameter(torch.FloatTensor(size=(node_feat_dim*2,out_dim)))\n",
        "linear=nn.Parameter(torch.FloatTensor(size=(3*2,1)))\n",
        "def concat_message_function(edges):\n",
        "  return{'cat_feat':torch.cat([edges.src['x'],edges.dst['x']],dim=1)}\n",
        "g.apply_edges(concat_message_function)\n",
        "g.edata['out']=g.edata['cat_feat']@ linear\n",
        "print(g)\n",
        "print(g.ndata)\n",
        "print(g.edata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxiJatDv89Vi",
        "outputId": "6e5bed5c-cf56-45f7-df99-1e368e5ceb78"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph(num_nodes=4, num_edges=4,\n",
            "      ndata_schemes={'x': Scheme(shape=(3,), dtype=torch.float32)}\n",
            "      edata_schemes={'w': Scheme(shape=(), dtype=torch.float32), 'cat_feat': Scheme(shape=(6,), dtype=torch.float32), 'out': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "{'x': tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])}\n",
            "{'w': tensor([0.1000, 0.6000, 0.9000, 0.7000]), 'cat_feat': tensor([[1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.]]), 'out': tensor([[5.1014e-31],\n",
            "        [5.1014e-31],\n",
            "        [5.1014e-31],\n",
            "        [5.1014e-31]], grad_fn=<MmBackward0>)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The suggested implementation splits the linear operation into two, one applies on src feature, the other applies on dst feature. It then adds the output of the linear operations on the edges at the final stage,i.e. performing $W_l\\times u+W_r\\times v.$This is beacuse $W\\times (u||v)=W_l\\times u+W_r\\times v$,where $W_l$ and $W_r$ are the left and the right half of the matrix W,repectively:"
      ],
      "metadata": {
        "id": "Rk5yE2urCA3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "#import dgl.functions as fn\n",
        "\n",
        "#linear_src=nn.Parameter(torch.FloatTensor(size=(node_feat_dim,out_dim)))\n",
        "#linear_dst=nn.Parameter(torch.FloatTensor(size=(node_feat_dim,out_dim)))\n",
        "#out_src=g.ndata['feat']@linear_src\n",
        "#out_dst=g.ndata['feat']@linear_dst\n",
        "#g.dstdata.update({'out_dst':out_dst})\n",
        "#g.apply_edges(fn.u_add_v('out_src','out_dst','out'))"
      ],
      "metadata": {
        "id": "I2QK_HhaDHGQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl.function as fn\n",
        "\n",
        "u,v=torch.tensor([0,1,2,3]),torch.tensor([2,0,0,1])\n",
        "weights1=torch.tensor([0.1,0.6,0.9,0.7]) #weight of each edge\n",
        "gg=dgl.graph((u,v))\n",
        "gg.edata['w']=weights\n",
        "gg.ndata['x']=torch.ones(gg.num_nodes(),3)\n",
        "\n",
        "linear_src=nn.Parameter(torch.FloatTensor(size=(3,1)))\n",
        "linear_dst=nn.Parameter(torch.FloatTensor(size=(3,1)))\n",
        "out_src=gg.ndata['x']@ linear_src\n",
        "out_dst=gg.ndata['x']@ linear_dst\n",
        "gg.srcdata.update({'out_src':out_src})\n",
        "gg.dstdata.update({'out_dst':out_dst})\n",
        "gg.apply_edges(fn.u_add_v('out_src','out_dst','out'))\n",
        "print(gg)\n",
        "print(gg.ndata)\n",
        "print(gg.edata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1drDLH_SEMlo",
        "outputId": "0925c288-1576-48d1-8e95-473109a13f42"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph(num_nodes=4, num_edges=4,\n",
            "      ndata_schemes={'x': Scheme(shape=(3,), dtype=torch.float32), 'out_src': Scheme(shape=(1,), dtype=torch.float32), 'out_dst': Scheme(shape=(1,), dtype=torch.float32)}\n",
            "      edata_schemes={'w': Scheme(shape=(), dtype=torch.float32), 'out': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "{'x': tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]]), 'out_src': tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<MmBackward0>), 'out_dst': tensor([[5.1014e-31],\n",
            "        [5.1014e-31],\n",
            "        [5.1014e-31],\n",
            "        [5.1014e-31]], grad_fn=<MmBackward0>)}\n",
            "{'w': tensor([0.1000, 0.6000, 0.9000, 0.7000]), 'out': tensor([[5.1014e-31],\n",
            "        [5.1014e-31],\n",
            "        [5.1014e-31],\n",
            "        [5.1014e-31]], grad_fn=<GSDDMMBackward>)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above two implementations are mathematically equivalent. The latter one is more efficient because it does not need to save feat_src and feat_dst on edges, which is not memory-efficient. Plus, addition could be optimized with DGL’s built-in function u_add_v(), which further speeds up computation and saves memory footprint."
      ],
      "metadata": {
        "id": "eScsE2Q0Gw77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 Apply Message Passing On Part Of The Graph\n",
        "\n",
        "If one only wants to update part of the nodes in the graph, the practice is to create a subgraph by providing the IDs for the nodes to include in the update, then call update_all() on the subgraph. For example:"
      ],
      "metadata": {
        "id": "1MrvVPb2J1V9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nid=[0,2,3,6,7,9]\n",
        "#sg=g.subgraph(nid)\n",
        "#sg.updata_all(message_func,reduce_func,apply_node_func)"
      ],
      "metadata": {
        "id": "QZodmd_0KpE5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a common usage in mini-batch training.Check Chapter 6: Stochastic Training on Large Graphs for more detailed usages.\n",
        "\n"
      ],
      "metadata": {
        "id": "AfVGYhINKuSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 Message Passing on Heterogeneous Graph"
      ],
      "metadata": {
        "id": "tIwIGW6mlrQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heterogeneous graphs (1.5 Heterogeneous Graphs), or heterographs for short, are graphs that contain different types of nodes and edges. The different types of nodes and edges tend to have different types of attributes that are designed to capture the characteristics of each node and edge type. Within the context of graph neural networks, depending on their complexity, certain node and edge types might need to be modeled with representations that have a __different number of dimensions.__\n",
        "\n",
        "The message passing on heterographs can be split into two parts:\n",
        "\n",
        "*  Message computation and aggregation for each relation r.\n",
        "\n",
        "*  Reduction that merges the aggregation results from all relations for each node type.\n",
        "\n",
        "DGL’s interface to call message passing on heterographs is multi_update_all(). multi_update_all() takes a dictionary containing the parameters for update_all() within each relation using relation as the key, and a string representing the cross type reducer. The reducer can be one of sum, min, max, mean, stack. Here’s an example:\n"
      ],
      "metadata": {
        "id": "JcPTTMZllwu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "不太懂wuwuwu......"
      ],
      "metadata": {
        "id": "uizCGaw_tDPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import dgl.function as fn\n",
        "import torch as th\n",
        "\n",
        "graph_data={\n",
        "    ('drug','interacts','drug'):(th.tensor([0,1]),th.tensor([1,2])), # 0-1,1-2\n",
        "    ('drug','interacts','gene'):(th.tensor([0,1]),th.tensor([2,3])), # 0-2,1-3\n",
        "    ('drug','treats','disease'):(th.tensor([1]),th.tensor([2])) # 1-2\n",
        "}\n",
        "G=dgl.heterograph(graph_data)\n",
        "\n",
        "for c_etype in G.canonical_etypes:\n",
        "  srctype,etype,dsttype=c_etype\n",
        "  wh=self.weight[etype](feat_dict[srctype])\n",
        "  # save it in graph for message passing\n",
        "  G.nodes[srctype].data['wh_%s'% etype]=wh\n",
        "  # specify per_relation message passing functions:(message_func,reduce_func).\n",
        "  # note that the results are saved to the same destination feature 'h',\n",
        "  # which hints the type wise reducer for aggregation.\n",
        "  funcs[etype]=(fn.copy_u('wh_%s' % etype,'m'),fn.mean('m','h'))\n",
        "# Trigger message passing of multiple types\n",
        "G.multi_update_all(funcs,'sum')\n",
        "# return the updated node feature dictionary\n",
        "return {ntype:G.nodes[ntype].data['h'] for ntype in G.ntypes}'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "iaM5hyxkoSDV",
        "outputId": "af18d5c4-a710-4523-9d65-a090dd7f777b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"import dgl.function as fn\\nimport torch as th\\n\\ngraph_data={\\n    ('drug','interacts','drug'):(th.tensor([0,1]),th.tensor([1,2])), # 0-1,1-2\\n    ('drug','interacts','gene'):(th.tensor([0,1]),th.tensor([2,3])), # 0-2,1-3\\n    ('drug','treats','disease'):(th.tensor([1]),th.tensor([2])) # 1-2\\n}\\nG=dgl.heterograph(graph_data)\\n\\nfor c_etype in G.canonical_etypes:\\n  srctype,etype,dsttype=c_etype\\n  wh=self.weight[etype](feat_dict[srctype])\\n  # save it in graph for message passing\\n  G.nodes[srctype].data['wh_%s'% etype]=wh\\n  # specify per_relation message passing functions:(message_func,reduce_func).\\n  # note that the results are saved to the same destination feature 'h',\\n  # which hints the type wise reducer for aggregation.\\n  funcs[etype]=(fn.copy_u('wh_%s' % etype,'m'),fn.mean('m','h'))\\n# Trigger message passing of multiple types\\nG.multi_update_all(funcs,'sum')\\n# return the updated node feature dictionary\\nreturn {ntype:G.nodes[ntype].data['h'] for ntype in G.ntypes}\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}